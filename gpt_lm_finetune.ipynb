{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPT2LMHeadModel for the language model \n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained model\n",
    "\n",
    "model parameters are downloaded from https://huggingface.co/gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('data_disk/gpt2/')\n",
    "tokenizer.pad_token = tokenizer.eos_token   #   set the padding token\n",
    "model = GPT2LMHeadModel.from_pretrained('data_disk/gpt2/', pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    'This is a sentence',\n",
    "    '随便写点什么'\n",
    "    'GPT-2 is a transformers model pretrained on a very large corpus of', \n",
    "    'English data in a self-supervised fashion. This means it was pretrained',\n",
    "    'on the raw texts only, with no humans labelling them in any way (which is',\n",
    "    'why it can use lots of publicly available data) with an',\n",
    "    'automatic process to generate inputs and labels from those texts.',\n",
    "    'More precisely, it was trained to guess the next word in sentences.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个函数用来在dataloader里面把数据先处理好\n",
    "def collate_fn(batch: list[str]):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        batch, \n",
    "        max_length=512,         # 最多512tokens，多的truncate\n",
    "        padding=True,           # padding成一样长\n",
    "        truncation=True,        # 砍掉太长的\n",
    "        return_tensors='pt',    # 返回pytorch tensor\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the dataloader\n",
    "loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,           # 随机打乱数据集\n",
    "    collate_fn=collate_fn,  # 把str转换成tensor的函数\n",
    "    num_workers=4,          # 在模型训练的时候多进程并行处理好数据\n",
    "    prefetch_factor=2,      # 提前处理好两个batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)  # 推荐用AdamW优化器，这一步之前可以吧模型 to gpu/tpu\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.035940170288086\n",
      "6.419832706451416\n",
      "6.076235294342041\n",
      "5.741185188293457\n",
      "5.414309501647949\n",
      "5.130519390106201\n",
      "4.865398406982422\n",
      "4.583464622497559\n",
      "4.283649444580078\n",
      "3.976851224899292\n",
      "3.669083595275879\n",
      "3.3634867668151855\n",
      "3.0705478191375732\n",
      "2.8095779418945312\n",
      "2.5956103801727295\n",
      "2.4250950813293457\n",
      "2.284105062484741\n",
      "2.1639373302459717\n",
      "2.0620718002319336\n",
      "1.9774707555770874\n",
      "1.9081190824508667\n",
      "1.850902795791626\n",
      "1.8023258447647095\n",
      "1.7592434883117676\n",
      "1.719598412513733\n",
      "1.6820510625839233\n",
      "1.6457133293151855\n",
      "1.610085129737854\n",
      "1.5749411582946777\n",
      "1.5401504039764404\n",
      "1.5056320428848267\n",
      "1.4713009595870972\n",
      "1.4371025562286377\n",
      "1.4029947519302368\n",
      "1.3689444065093994\n",
      "1.3349045515060425\n",
      "1.3007946014404297\n",
      "1.2665187120437622\n",
      "1.23199462890625\n",
      "1.197184681892395\n",
      "1.1621410846710205\n",
      "1.1270976066589355\n",
      "1.0923007726669312\n",
      "1.0577248334884644\n",
      "1.0231841802597046\n",
      "0.9884929060935974\n",
      "0.9538183212280273\n",
      "0.9198257923126221\n",
      "0.8866767287254333\n",
      "0.8537225127220154\n",
      "0.8203688859939575\n",
      "0.7863523364067078\n",
      "0.751674234867096\n",
      "0.7164652943611145\n",
      "0.6805561780929565\n",
      "0.6437732577323914\n",
      "0.6062347888946533\n",
      "0.5697365999221802\n",
      "0.5355352759361267\n",
      "0.5031915307044983\n",
      "0.47229450941085815\n",
      "0.4431321620941162\n",
      "0.4172516167163849\n",
      "0.3950633704662323\n",
      "0.3750632405281067\n",
      "0.35642147064208984\n",
      "0.33897149562835693\n",
      "0.32274284958839417\n",
      "0.30772364139556885\n",
      "0.2937811315059662\n",
      "0.2807195484638214\n",
      "0.26835352182388306\n",
      "0.2565276324748993\n",
      "0.24503548443317413\n",
      "0.2337532788515091\n",
      "0.22296033799648285\n",
      "0.21270224452018738\n",
      "0.20295831561088562\n",
      "0.1938413679599762\n",
      "0.18531441688537598\n",
      "0.17731735110282898\n",
      "0.16981370747089386\n",
      "0.16278107464313507\n",
      "0.1561952531337738\n",
      "0.15001672506332397\n",
      "0.14419861137866974\n",
      "0.1386907994747162\n",
      "0.13345275819301605\n",
      "0.12844859063625336\n",
      "0.12365356087684631\n",
      "0.11905014514923096\n",
      "0.1146257072687149\n",
      "0.11036913096904755\n",
      "0.1062728762626648\n",
      "0.10232941061258316\n",
      "0.09852984547615051\n",
      "0.09486424922943115\n",
      "0.09131806343793869\n",
      "0.08787823468446732\n",
      "0.08453042805194855\n",
      "0.08126291632652283\n",
      "0.07806691527366638\n",
      "0.07493715733289719\n",
      "0.07187025994062424\n",
      "0.06886545568704605\n",
      "0.06592434644699097\n",
      "0.06305110454559326\n",
      "0.060251664370298386\n",
      "0.05753357335925102\n",
      "0.05490497127175331\n",
      "0.05237658694386482\n",
      "0.04995812103152275\n",
      "0.04765743389725685\n",
      "0.04548175632953644\n",
      "0.04343496635556221\n",
      "0.041517216712236404\n",
      "0.03972417116165161\n",
      "0.03804999217391014\n",
      "0.036486536264419556\n",
      "0.03502483665943146\n",
      "0.033656805753707886\n",
      "0.03237538039684296\n",
      "0.031173622235655785\n",
      "0.03004555031657219\n",
      "0.02898593619465828\n",
      "0.027990106493234634\n",
      "0.027052607387304306\n",
      "0.026169903576374054\n",
      "0.025337733328342438\n",
      "0.024551963433623314\n",
      "0.02380971610546112\n",
      "0.02310672029852867\n",
      "0.02244088053703308\n",
      "0.021808519959449768\n",
      "0.021207313984632492\n",
      "0.02063480019569397\n",
      "0.0200887992978096\n",
      "0.01956729032099247\n",
      "0.019068032503128052\n",
      "0.018589595332741737\n",
      "0.01813039369881153\n",
      "0.0176888145506382\n",
      "0.01726365275681019\n",
      "0.016853734850883484\n",
      "0.01645798608660698\n",
      "0.016075562685728073\n",
      "0.015705112367868423\n",
      "0.015346513129770756\n",
      "0.014998702332377434\n",
      "0.014661279506981373\n",
      "0.014333488419651985\n",
      "0.014015104621648788\n",
      "0.013705466873943806\n",
      "0.013404707424342632\n",
      "0.013111904263496399\n",
      "0.01282745972275734\n",
      "0.012550798244774342\n",
      "0.012281723320484161\n",
      "0.01202036440372467\n",
      "0.011766199953854084\n",
      "0.011518951505422592\n",
      "0.011278779245913029\n",
      "0.011045272462069988\n",
      "0.010818506591022015\n",
      "0.010597918182611465\n",
      "0.01038355752825737\n",
      "0.010174959897994995\n",
      "0.009971861727535725\n",
      "0.009774408303201199\n",
      "0.00958232395350933\n",
      "0.009395086206495762\n",
      "0.009212782606482506\n",
      "0.009035185910761356\n",
      "0.008861985988914967\n",
      "0.008693189360201359\n",
      "0.008528637699782848\n",
      "0.008368093520402908\n",
      "0.008211363106966019\n",
      "0.008058440871536732\n",
      "0.007909179665148258\n",
      "0.007763623725622892\n",
      "0.007621340453624725\n",
      "0.00748251611366868\n",
      "0.007346850819885731\n",
      "0.00721447728574276\n",
      "0.00708508538082242\n",
      "0.006958655547350645\n",
      "0.0068353707902133465\n",
      "0.0067147500813007355\n",
      "0.006596899591386318\n",
      "0.006481715012341738\n",
      "0.006369132548570633\n",
      "0.006259167101234198\n",
      "0.006151464302092791\n",
      "0.006046339403837919\n",
      "0.005943471100181341\n",
      "0.005842800252139568\n",
      "0.005744435824453831\n",
      "0.005648152437061071\n",
      "0.005553994793444872\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "for e in range(epochs):\n",
    "    for input in loader:\n",
    "        # 如果模型在gpu，这里要把input里面的matrix也to gpu\n",
    "        input['input_ids'] = input['input_ids'].to(device)\n",
    "        input['attention_mask'] = input['attention_mask'].to(device)\n",
    "\n",
    "        input['labels'] = input['input_ids']    # 语言模型训练，label就是输入id\n",
    "        \n",
    "        output = model(**input)\n",
    "        loss = output.loss\n",
    "        loss.backward()  # loss 就是 negative loglikelihood\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
